---
layout: post
published: true
title: Jak nechat jazykovÃ½ model pÅ™emÃ½Å¡let a pÅ™i tom pouÅ¾Ã­vat nÃ¡stroje podle potÅ™eby s o4-mini. ZaÄnou modely uvaÅ¾ovat v ne-lidÅ¡tinÄ›?
tags:
- AI
---
Chci se dnes vÄ›novat nÄ›Äemu, co zdÃ¡ se mi obÄas lidem ne plnÄ› dochÃ¡zÃ­ - reasoning modely o3 a o4-mini mohou pÅ™istupovat k nÃ¡strojÅ¯m opakovanÄ› v prÅ¯bÄ›hu svÃ©ho pÅ™emÃ½Å¡lenÃ­. To hodnÄ› zajÃ­mavÃ½ rozdÃ­l oproti pÅ™edeÅ¡lÃ© generaci, tedy o3-mini a o1. PodÃ­vejme se proÄ.

# ChovÃ¡nÃ­ o3-mini vs. o4-mini
NÃ¡stroje jsou ÄÃ­m dÃ¡l zÃ¡sadnÄ›jÅ¡Ã­ pro vÃ½sledky vaÅ¡ich AI aplikacÃ­ a jsou jÃ¡drem toho, Äemu dnes obvykle lidÃ© Å™Ã­kajÃ­ AI Agent. NÃ¡stroje zahrnujÃ­ napÅ™Ã­klad vyhledÃ¡vÃ¡nÃ­ na Internetu, vyhledÃ¡vÃ¡nÃ­ v databÃ¡zi dokumentÅ¯ (RAG), pokroÄilÃ© datovÃ© dotazy do klasickÃ½ch databÃ¡zÃ­, spouÅ¡tÄ›nÃ­ AI generovanÃ©ho poÄÃ­taÄovÃ©ho kÃ³du, ovlÃ¡dÃ¡nÃ­ poÄÃ­taÄe nebo provolÃ¡vÃ¡nÃ­ nÄ›jakÃ½ch API a tak podobnÄ›. 

**o3-mini** funguje tak, Å¾e na dotaz uÅ¾ivatele posoudÃ­, zda bude vhodnÃ© volat nÄ›jakÃ© nÃ¡stroje, kterÃ½ch mÅ¯Å¾e vybrat hned nÄ›kolik, a jakÃ© parametry bude potÅ™eba nÃ¡strojÅ¯m pÅ™edat. NÃ¡slednÄ› vÃ¡Å¡ kÃ³d (nebo serverovÃ¡ strana u zabudovanÃ½ch nÃ¡strojÅ¯ tÅ™eba v Responses API) nÃ¡stroje provolÃ¡, vÃ½stupy pÅ™ipravÃ­ a pÅ™edÃ¡ je modelu, kterÃ½ zaÄne pÅ™emÃ½Å¡let (reasoning) a kdyÅ¾ si vÅ¡e rozmyslÃ­, vygeneruje odpovÄ›Ä. Ta "agency", tedy schopnost se rozhodovat, je na tom zaÄÃ¡tku, kdy se rozhodne kterÃ© nÃ¡stroje a jak pouÅ¾Ã­t.

UrÄitÄ› jste ale narazili na pokroÄilejÅ¡Ã­ agenty jako je **deep research**. Tam se dÄ›je vÃ­c vÄ›cÃ­, ale jedna je zÃ¡sadnÃ­ - agent nefunguje jen na principu otÃ¡zka a nÃ¡slednÃ¡ odpovÄ›Ä, ale je to cyklickÃ½ proces, kdy agent se rozhoduje, co udÄ›lat dÃ¡l. Tak napÅ™Ã­klad vybere nÃ¡stroje, pÅ™eÄte si odpovÄ›di, uvaÅ¾uje a sestavÃ­ seznam oblastÃ­, kterÃ© pro odpovÄ›Ä hrajÃ­ zÃ¡sadnÃ­ roli. NÃ¡Å¡ kÃ³d by pak nÃ¡slednÄ› vzal tyto oblasti a pro kaÅ¾dou z nich znovu zavolal o3-mini. To by pro kaÅ¾dou tuto vÄ›tev urÄilo vhodnÃ© nÃ¡stroje (napÅ™Ã­klad vyhledÃ¡nÃ­ informacÃ­ o tÃ©to oblasti na Internetu), sbÄ›r vÃ½sledkÅ¯, uvaÅ¾ovÃ¡nÃ­, odpovÄ›Ä. NÃ¡slednÄ› v kÃ³du vezmeme vÅ¡echny odpovÄ›di a znovu zavolÃ¡me o3-mini, aÅ¥ si vÅ¡echny oblasti proÄte a provede syntÃ©zu zÃ¡vÄ›reÄnÃ© odpovÄ›di. NÃ¡Å¡ kÃ³d funguje jako **orchestrÃ¡tor** a celÃ½ tento proces vede na hlubÅ¡Ã­, pÅ™esnÄ›jÅ¡Ã­ a komplexnÄ›jÅ¡Ã­ odpovÄ›di moÅ¾nÃ¡ aÅ¾ na Ãºrovni deep research v rÅ¯znÃ½ch produktech.

**o4-mini** dÄ›lÃ¡ ale jednu vÄ›c jinak. DokÃ¡Å¾e si Å™Ã­kat o nÃ¡stroje **v prÅ¯bÄ›hu uvaÅ¾ovÃ¡nÃ­**, ne jen jednou na zaÄÃ¡tku. MÅ¯Å¾e uvaÅ¾ovat a toÄit nÄ›kolik cyklÅ¯ dokud nenÃ­ spokojen a teprve pak generovat odpovÄ›Ä. SamotnÃ½ reasoning modelu je tedy sÃ¡m o sobÄ› **"agentic"**, prÅ¯bÄ›Å¾nÄ› vyhodnocuje situaci a Å¾Ã¡dÃ¡ o provolÃ¡vÃ¡nÃ­ nÃ¡strojÅ¯ dle aktuÃ¡lnÃ­ho stavu jeho uvaÅ¾ovÃ¡nÃ­. Jinak Å™eÄeno **reasoning je orchestrÃ¡torem**. 

Technicky budeme pouÅ¾Ã­vat Responses API, ale jÃ¡ chci jÃ­t do vlastnÃ­ho nÃ¡stroje, takÅ¾e se jeho pouÅ¾Ã­vÃ¡nÃ­ nebude odehrÃ¡vat na serverovÃ© stranÄ›. To je vyÅ™eÅ¡eno pÅ™es **streaming** - model mi servÃ­ruje udÃ¡losti, napÅ™Ã­klad pÅ™emÃ½Å¡lenÃ­, nÃ¡stroje, generovÃ¡nÃ­ parametrÅ¯ nÃ¡strojÅ¯ Äi finÃ¡lnÃ­ch odpovÄ›dÃ­ token po tokenu. 

# ExtrÃ©mnÃ­ pÅ™Ã­klad, kde se to dobÅ™e ukÃ¡Å¾e
Jak si jednoduÅ¡e ukÃ¡zat situaci, kdy iterativnÃ­ pouÅ¾Ã­vÃ¡nÃ­ nÃ¡strojÅ¯ model bude opravdu potÅ™ebovat a simulovat tak scÃ©nÃ¡Å™ sloÅ¾itÃ©ho zkoumÃ¡nÃ­? VytvoÅ™il jsem si nÃ¡stroj, kterÃ½ vracÃ­ "nÃ¡sledujÃ­cÃ­ vhodnÃ© mÄ›sto" k nÃ¡vÅ¡tÄ›vÄ› na dovolenÃ© Äi chcete-li cestÄ› kolem svÄ›ta. NÃ¡stroj oÄekÃ¡vÃ¡ na vstupu buÄ <START> kdyÅ¾ jede od zaÄÃ¡tku, nebo nÄ›jakÃ© mÄ›sto po cestÄ› a vracÃ­ mÄ›sto nÃ¡sledujÃ­cÃ­ pÅ™Ã­padnÄ› <END> pokud uÅ¾ nenÃ­ kam jet. Model bude mÃ­t Ãºkol zjistit celou trasu a je jasnÃ©, Å¾e v ten okamÅ¾ik mu nestaÄÃ­ rozhodnout o nÃ¡stroji jen jednou na zaÄÃ¡tku. MusÃ­ iterativnÄ› zjiÅ¡Å¥ovat co je dalÅ¡Ã­ho a zÃ­skanÃ© znalosti uplatÅˆovat pro dalÅ¡Ã­ hledÃ¡nÃ­. A abych mu to trochu znesnadnil, tak v promptu jeÅ¡tÄ› budu chtÃ­t aby aÅ¾ dojde na konec, tak ovÄ›Å™il sprÃ¡vnost trasy tÃ­m, Å¾e pÅ¯jde zase pozpÃ¡tku aÅ¾ do zaÄÃ¡tku.

CelÃ¡ ukÃ¡zka je [na mÃ©m GitHubu](https://github.com/tkubica12/azure-workshops/tree/main/d-ai-reasoning-with-tools)

Tady je kÃ³d:

```python
from openai import AzureOpenAI
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
from dotenv import load_dotenv
import os
import json
import itertools

# Load environment variables from .env file
load_dotenv()

# Get Azure OpenAI endpoint and model name from environment variables
AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT")
MODEL_NAME = os.getenv("MODEL_NAME")

# Set up Azure AD token provider for authentication
token_provider = get_bearer_token_provider(
    DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default"
)

# Initialize Azure OpenAI client
client = AzureOpenAI(
  azure_endpoint = AZURE_OPENAI_ENDPOINT, 
  azure_ad_token_provider=token_provider,
  api_version="2025-04-01-preview"
)

# Tool that model can use, it is static for demo purposes
def get_next_item(current_item):
    """
    Returns the next item in the static chain of cities to visit.
    Special items:
      - '<START>': returns the first item in the chain.
      - '<END>': returned if there is no next item.
    """
    chain = ["Prague", "Vienna", "Tokyo", "Bangkok", "Paris"]
    if current_item == "<START>":
        return chain[0]
    try:
        idx = chain.index(current_item)
        if idx + 1 < len(chain):
            return chain[idx + 1]
        else:
            return "<END>"
    except ValueError:
        return "<END>"

# Define available tools for the model
tools = [{
    "type": "function",
    "name": "get_next_item",
    "description": "Returns the next city in the chain of cities to visit",
    "parameters": {
        "type": "object",
        "properties": {
            "current_item": {
                "type": "string",
                "description": "The current city. Use '<START>' to get the first city."
            }
        },
        "required": ["current_item"],
        "additionalProperties": False
    }
}]

# Initialize response tracking variables
response_id = None
input_messages = [{"role": "user", "content": "Gather full chain of cities to visit. When using the tool you must make sure that it works by going back in opposite direction once you reach the end of the chain and this way verify results. Make sure to provide details about your reasoning on each step."}]

# Loop until the model stops calling tools
while True: 
    # Create a streaming response from the model
    response = client.responses.create(
        model=MODEL_NAME,
        input=input_messages,
        tools=tools,
        stream=True,
        store=True,
        reasoning={
            "effort":  "high",          # optional: low | medium | high
            "summary": "detailed",      # auto | concise | detailed
        },
    )

    # Collect tool outputs to send back after this streaming pass
    pending_outputs = []

    # Process streaming events from the model
    for event in response:
        if hasattr(event, "response_id"):
            response_id = event.response_id
        if event.type == 'response.output_text.delta':
            print(event.delta, end='', flush=True)
        elif event.type == 'response.function_call_arguments.delta':
            print(event.delta, end='', flush=True)
        elif event.type == 'response.output_item.added':
            if event.item.type == 'function_call':
                print(f"\nğŸ”§ Calling {event.item.name}, arguments: ", end='', flush=True)
            elif event.item.type == 'reasoning':
                print(f"\nğŸ§  Reasoning")
            elif event.item.type == 'message':
                print("\nğŸ’¬ Responding")
            else:
                print(f"[DEBUG] Added item: {event.item}")
        elif event.type == 'response.output_item.done':
            if event.item.type == 'function_call' and event.item.name == 'get_next_item':
                # 1. Execute tool
                current_item = json.loads(event.item.arguments)['current_item']
                next_item = get_next_item(current_item)
                print(f"\nğŸ”§ Function call result: {next_item}")

                # 2. Remember messages to send back in next request
                input_messages.append(event.item)                  # the call itself
                pending_outputs.append({                           # its result
                    "type": "function_call_output",
                    "call_id": event.item.call_id,
                    "output": next_item
                })
            elif event.item.type == 'reasoning':
                for seg in event.item.summary:
                    print("ğŸ§ ", seg.text)
                print("ğŸ§  Finished reasoning")
                input_messages.append(event.item)    # keep reasoning in history
            elif event.item.type == 'message':
                print("\n\nğŸ’¬ Finished responding.")
            else:
                print(f"\n[DEBUG] Done item: {event.item}")        
    # After stream ends, append all outputs (if any) and decide whether to loop again
    if not pending_outputs:
        break                                   # no more tool calls â†’ finished
    input_messages.extend(pending_outputs)      # add results, then loop

# Cleanup: delete the response if it exists
if response_id is not None:
    client.responses.delete(response_id)
```

DoporuÄuju si pustit u sebe, aÅ¥ vidÃ­te jak se to streamuje. K vysvÄ›tlenÃ­ kÃ³du:
- **get_next_item** - nÃ¡stroj, kterÃ½ vracÃ­ dalÅ¡Ã­ mÄ›sto v Å™etÄ›zci.
- **tools** - definice nÃ¡strojÅ¯, kterÃ© model mÅ¯Å¾e pouÅ¾Ã­t. V naÅ¡em pÅ™Ã­padÄ› je to jen jeden nÃ¡stroj, ale mÅ¯Å¾e jich bÃ½t vÃ­c a model si je mÅ¯Å¾e volit podle potÅ™eby. 
- **User message** - dÃ¡vÃ¡ za Ãºkol za pouÅ¾itÃ­ nÃ¡strojÅ¯ odhalit celou trasu a jeÅ¡tÄ› si ji ovÄ›Å™it zpÄ›tnÄ› tak, Å¾e projde znovu odzadu.
- **Responses API** - vÅ¡imnÄ›te si jednak, Å¾e registrujeme nÃ¡stroje, pouÅ¾Ã­vÃ¡me streaming a u reasoning jsem specificky zapnul jednak vysokÃ© ÃºsilÃ­ (jsem ochoten platit za vÃ­c pÅ™emÃ½Å¡lecÃ­ch tokenÅ¯) a jednak poskytovÃ¡nÃ­ detailnÃ­ho shrnutÃ­ pÅ™emÃ½Å¡lecÃ­ch krokÅ¯. Na rozdÃ­l od open source modelÅ¯, kde si mÅ¯Å¾ete pÅ™eÄÃ­st skuteÄnÃ© thinking tokeny, tady jde jen o bezpeÄnÃ© shrnutÃ­, ne tokeny samotnÃ©. PÅ™edpoklÃ¡dÃ¡m, Å¾e dÅ¯vodÅ¯ je nÄ›kolik, ale hlavnÃ­ dva budou duÅ¡evnÃ­ vlastnictvÃ­ (prÃ¡vÄ› ten pÅ™emÃ½Å¡lecÃ­ proces je to nejcennÄ›jÅ¡Ã­ - proto tÅ™eba Deepseek R1 nenÃ­ zajÃ­mavÃ½ jen z pohledu toho 671B modelu, kterÃ½ je obrovskÃ½ a zcela mimo hranice domÃ¡cÃ­ho hardware a to minimÃ¡lnÄ› 20x, ale i proto, Å¾e jeho pÅ™emÃ½Å¡lecÃ­ proces lze pouÅ¾Ã­t k dotrÃ©novÃ¡nÃ­ menÅ¡Ã­ch modelÅ¯) a druhÃ½ dÅ¯vod je poÄÃ­tÃ¡m "bezpeÄnostnÃ­" (pÅ™emÃ½Å¡lenÃ­ nenÃ­ cenzurovanÃ© a mÅ¯Å¾e zahrnovat vÄ›ci, kterÃ© jsou pro komerÄnÃ­ model nepÅ™Ã­pustnÃ© aÅ¥ by Å¡lo o nÃ¡silÃ­, politiku, rasismus a tak podobnÄ›).
- **Eventy** - streamovanÃ© udÃ¡losti, kterÃ© model generuje. VÅ¡imnÄ›te si, Å¾e mÃ¡me nÄ›kolik typÅ¯ udÃ¡lostÃ­. NapÅ™Ã­klad `response.output_text.delta` je text, kterÃ½ model generuje a `response.function_call_arguments.delta` je generovÃ¡nÃ­ argumentÅ¯ pro nÃ¡stroj. Pak mÃ¡me udÃ¡losti, kterÃ© oznaÄujÃ­ zaÄÃ¡tek a konec nÃ¡stroje, pÅ™emÃ½Å¡lenÃ­ a odpovÄ›di. Jedeme ve smyÄce a shÃ¡nÃ­me modelu odpovÄ›di na nÃ¡stroje, o kterÃ© si Å™ekne, dokud se model nerozhodne, Å¾e to staÄÃ­ a vygeneruje zÃ¡vÄ›reÄnou odpovÄ›Ä. 

PodÃ­vejme na vÃ½stup celÃ©ho procesu a uvaÅ¾ovÃ¡nÃ­ modelu. Dojde tam asi ke dvanÃ¡cti postupnÃ½m volÃ¡nÃ­m nÃ¡stroje a sice ne vÅ¾dy, ale obÄas o4-mini prozradÃ­ jak o tom pÅ™emÃ½Å¡lÃ­. FascinujÃ­cÃ­, kdyÅ¾ si model pomyslÃ­ tÅ™eba tohle:

*MÅ¯j dalÅ¡Ã­ krok je zavolat dalÅ¡Ã­ mÄ›sto po â€Tokiuâ€œ. Jsem zvÄ›davÃ½, co to bude! PÅ™ipadÃ¡ mi to jako malÃ© dobrodruÅ¾stvÃ­ sbÃ­rat nÃ¡zvy tÄ›chto mÄ›st a jsem nadÅ¡enÃ½, Å¾e mohu pokraÄovat. UvidÃ­me, co pÅ™inese dalÅ¡Ã­. TÄ›Å¡Ã­m se, Å¾e budu sbÃ­rat dalÅ¡Ã­!*

```markdown

ğŸ§  Reasoning
ğŸ§  Finished reasoning

ğŸ”§ Calling get_next_item, arguments: {"current_item":"<START>"}
ğŸ”§ Function call result: Prague

ğŸ§  Reasoning
ğŸ§  Finished reasoning

ğŸ”§ Calling get_next_item, arguments: {"current_item":"Prague"}
ğŸ”§ Function call result: Vienna

ğŸ§  Reasoning
ğŸ§  Finished reasoning

ğŸ”§ Calling get_next_item, arguments: {"current_item":"Vienna"}
ğŸ”§ Function call result: Tokyo

ğŸ§  Reasoning
ğŸ§  **Proceeding with city collection**

I've got "Tokyo" added to my list of cities, which now includes Prague, Vienna, and Tokyo. My next step is to call for the next city after "Tokyo." I'm curious to see what it will be! It feels like a little adventure gathering these city names, and I'm eager to keep going. Let's see what the next one will bring. I'm excited to collect more!
ğŸ§  Finished reasoning

ğŸ”§ Calling get_next_item, arguments: {"current_item":"Tokyo"}
ğŸ”§ Function call result: Bangkok

ğŸ§  Reasoning
ğŸ§  Finished reasoning

ğŸ”§ Calling get_next_item, arguments: {"current_item":"Bangkok"}
ğŸ”§ Function call result: Paris

ğŸ§  Reasoning
ğŸ§  Finished reasoning

ğŸ”§ Calling get_next_item, arguments: {"current_item":"Paris"}
ğŸ”§ Function call result: <END>

ğŸ§  Reasoning
ğŸ§  **Verifying city sequence**

The function returned "<END>", indicating I've completed forward traversal with cities: Prague, Vienna, Tokyo, Bangkok, and Paris. Now, I need to verify this sequence in reverse. Iâ€™ll check that each preceding city returns the correct next one, excluding the end. Beginning with Bangkok, Iâ€™ll see that it points to Paris, and continue with Tokyo to Bangkok, Vienna to Tokyo, and Prague to Vienna. Finally, confirming that Paris is last seems essential.      
ğŸ§  **Verifying links in reverse**

I need to go back through the cities I collected to verify the results by calling get_next_item. It seems like I should start from the end, with Paris, and work backwards to check each connection. For each city, I'll expect the correct next city based on the order: Bangkok should point to Paris, Tokyo to Bangkok, Vienna to Tokyo, Prague to Vienna, and finally, <START> should lead to Prague. I could also verify that Paris points to <END> for an extra confirmation.
ğŸ§  **Setting up backward verification**

Iâ€™ll do backward verification by checking the links again. Weâ€™ll call get_next_item for each city, starting with the last one, Paris. For Paris, I'll expect <END>. Then I'll proceed through Bangkok expecting Paris, Tokyo expecting Bangkok, Vienna expecting Tokyo, Prague expecting Vienna, and finally checking <START> expecting Prague. So, my first step is to verify Paris returns <END>. Then, I'll continue with the rest of the cities to validate the links. Let's start!
ğŸ§  Finished reasoning

ğŸ”§ Calling get_next_item, arguments: {"current_item":"Paris"}
ğŸ”§ Function call result: <END>

ğŸ§  Reasoning
ğŸ§  Finished reasoning

ğŸ”§ Calling get_next_item, arguments: {"current_item":"Bangkok"}
ğŸ”§ Function call result: Paris

ğŸ§  Reasoning
ğŸ§  Finished reasoning

ğŸ”§ Calling get_next_item, arguments: {"current_item":"Tokyo"}
ğŸ”§ Function call result: Bangkok

ğŸ§  Reasoning
ğŸ§  Finished reasoning

ğŸ”§ Calling get_next_item, arguments: {"current_item":"Vienna"}
ğŸ”§ Function call result: Tokyo

ğŸ§  Reasoning
ğŸ§  Finished reasoning

ğŸ”§ Calling get_next_item, arguments: {"current_item":"Prague"}
ğŸ”§ Function call result: Vienna

ğŸ§  Reasoning
ğŸ§  Finished reasoning

ğŸ”§ Calling get_next_item, arguments: {"current_item":"<START>"}
ğŸ”§ Function call result: Prague

ğŸ§  Reasoning
ğŸ§  Finished reasoning

ğŸ’¬ Responding
Here is the full chain of cities and a detailed account of each step, including forward retrieval and backward verification.

Forward Retrieval  
1. Step 1: call get_next_item("<START>") to get the first city  
   â€¢ Result: â€œPragueâ€  
2. Step 2: call get_next_item("Prague")  
   â€¢ Result: â€œViennaâ€  
3. Step 3: call get_next_item("Vienna")  
   â€¢ Result: â€œTokyoâ€  
4. Step 4: call get_next_item("Tokyo")  
   â€¢ Result: â€œBangkokâ€  
5. Step 5: call get_next_item("Bangkok")  
   â€¢ Result: â€œParisâ€  
6. Step 6: call get_next_item("Paris")  
   â€¢ Result: â€œ<END>â€ â€” indicates weâ€™ve reached the end of the chain

Collected chain (in order):  
1. Prague  
2. Vienna  
3. Tokyo  
4. Bangkok  
5. Paris  

Backward Verification  
Starting from the end, we call get_next_item on each preceding city to confirm it yields the correct successor:

1. Verify last city: call get_next_item("Paris") â†’ â€œ<END>â€ (expected)  
2. call get_next_item("Bangkok") â†’ â€œParisâ€ (matches)  
3. call get_next_item("Tokyo") â†’ â€œBangkokâ€ (matches)  
4. call get_next_item("Vienna") â†’ â€œTokyoâ€ (matches)  
5. call get_next_item("Prague") â†’ â€œViennaâ€ (matches)  
6. call get_next_item("<START>") â†’ â€œPragueâ€ (matches)

All expected outputs were confirmed. The chain is valid.

Final confirmed itinerary of cities to visit:  
Prague â†’ Vienna â†’ Tokyo â†’ Bangkok â†’ Paris

ğŸ’¬ Finished responding.
```

# ZÃ¡vÄ›r
PÅ™ed rokem jste museli bÄ›Å¾nÃ½ model promptem instruovat, aÅ¥ to bere "krok za krokem" a ohÃ½bat ho, aby se pokouÅ¡el si vÄ›ci rozmÃ½Å¡let, coÅ¾ sice reasoning zlepÅ¡ovalo, ale ne tak vÃ½raznÄ›. LepÅ¡Ã­ch vÃ½sledkÅ¯ bylo moÅ¾nÃ© dosÃ¡hnout v multi-agent scÃ©nÃ¡Å™i, kdy jste nechali model mluvit v rÅ¯znÃ½ch rolÃ­ch sama se sebou a s rÅ¯znÃ½mi nÃ¡stroji nebo pouÅ¾ili pokroÄilou orchestraci jako u deep research. DneÅ¡nÃ­ schopnost nejet uvaÅ¾ovat, ale v prÅ¯bÄ›hu pÅ™emÃ½Å¡lenÃ­ volit nÃ¡stroje a prÅ¯bÄ›Å¾nÄ› zpracovÃ¡vat jejich vÃ½sledky, je zÃ¡sadnÃ­ krok dopÅ™edu. V jednÃ© "otÃ¡Äce" tak zvlÃ¡dnete mnohem vÃ­c i bez orchestrace. MyslÃ­m, Å¾e to mÅ¯Å¾e vÃ©st na o dost lepÅ¡Ã­ vÃ½sledky, protoÅ¾e umoÅ¾Åˆujete modelu zÅ¯stat ve svÃ©m "vnitÅ™nÃ­m svÄ›tÄ›", v "jazyce v hlavÄ›" a teprve po ukonÄenÃ­ uvaÅ¾ovÃ¡nÃ­ to pÅ™etavit do odpovÄ›di. Na Deepseek R1 jsme vidÄ›li, Å¾e pokud je modelu ponechÃ¡na volnost, uvaÅ¾uje podivnÃ½m mixem angliÄtiny a ÄÃ­nÅ¡tiny. OÄekÃ¡vÃ¡m, Å¾e v budoucnu vzniknou modely, jejichÅ¾ proces uvaÅ¾ovÃ¡nÃ­ nebude pouÅ¾Ã­vat lidskÃ½ jazyk, ale nÄ›co efektivnÄ›jÅ¡Ã­ho s ÄÃ­m pÅ™ijdou sami a do jazyka se to pÅ™eloÅ¾Ã­ aÅ¾ na vÃ½stupu. 