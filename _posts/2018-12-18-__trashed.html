---
layout: post
status: draft
published: false
title: 'Kubernetes prakticky: vystavování služeb přes objekt Service'
author:
  display_name: Tomáš Kubica
  login: tomas
  email: tkubica@centrum.cz
  url: ''
author_login: tomas
author_email: tkubica@centrum.cz
wordpress_id: 2282
wordpress_url: http://tomaskubica.cz/?p=2282
date: '2018-12-18 21:07:37 +0000'
date_gmt: '2018-12-18 20:07:37 +0000'
categories:
- Vsechny prispevky
- Docker a Kubernetes
tags:
- Kubernetes
---
<p>Aplikace v Kubernetes mají smysl pouze tehdy, když se k nim dá připojit. Deployment vám umožní běžet víc instancí kontejnerů a integrovaná service discovery umožní ostatním mikroslužbám tyto Pody najít. Předpokládám ale, že pro většinu situací nebudete chtít discovery provádět sami a uvítáte jednu virtuální IP, která bude zátěž rozhazovat na jednotlivé instance. A udělat službu dostupnou pro klienty mimo cluster? Jak se sticky session? Jaká bude zdrojová IP? A jak to vlastně celé funguje? Podívejme se dnes na Service v Kubernetes.<!--more--></p>
<h1>Koncept Service vs. vaše vlastní discovery vs. Ingress</h1>
<p>Objekt Deployment vám vytvoří a udržuje instance Podů a díky integrovanému service discovery může váš klient ve formě druhé mikroslužby všechny tyto instance najít. Dokážete tak implementovat vlastní mechanismus balancování. Většinou ale budete chtít, aby pro vás Kubernetes něco takového udělal sám.</p>
<p>Dnes se zaměříme na Service. Jde o vytvoření virtuální IP adresy pro vaše Pody, která pro vás zaregistruje DNS jméno uvnitř clusteru a bude zátěž rozhazovat. Je to implementované tak, že každý node clusteru si tuto IP "vezme". Ona ve skutečnosti není přiřazena žádnému konkrétnímu interface, ale je použita v iptables, které jsou pod kapotou (pracuje se i na ještě výkonější implementaci ipvs). O zakládání těchto pravidel se stará komponenta kube-proxy na každém nodu. Existuje víc režimů jak může Service fungovat a na ty se dnes podíváme. Podstatné ale je, že všechny níže předvedené implementace jsou L4. Nefungují tedy jako reverse proxy. Komunikace mezi klientem a Pody je přímá, celé je to tedy o směrování a případně NAT operacích. Věci jako certifikáty pro TLS tedy implementujete například v samotných Podech nebo naopak v nějaké WAFce ještě před celým clusterem. Service má řadu výhod. Není závislá na HTTP (funguje pro jakýkoli protokol), dosahuje přímé komunikace klienta s vaším kódem a díky tomu má také po cestě méně komponent, což přispívá k nižší latenci.</p>
<p>Alternativou je použití Ingress, o kterém si řekneme někdy příště. Jedná se o L7 řešení, tedy reverse proxy. Díky tomu můžete provádět věci jako je URL směrování (podle cesty - například /images půjde jinam než /catalog) nebo TLS terminaci. Pro vystavování web serverů je to jistě velmi elegantní a ucelené řešení, proto se k němu v jiném článku vrátíme.</p>
<h1>Volné vázání Service a Podů</h1>
<p>Důležitým konceptem je to, že přiřazení Podů k Service není pevné hierarchické, ale využívá se selekce přes metadata. U Podů máte definované labely a právě ty používá Service pro identifikaci těch Podů, které jsou její součástí. To je velmi praktické. Můžete například vytvořit Service a ještě v ní žádné Pody nemít. Dokážete třeba udělat dva Deploymenty a oba přiřadit pod stejnou službu (třeba pro canary release). O technikách nasazování a upgradování aplikací si řekneme podrobněji jindy.</p>
<h1>Interní služba s použitím ClusterIP</h1>
<p>Začneme nasazením služby pouze pro interní účely clusteru, tzv. typ ClusterIP. Takhle bude vypadat můj deployment. Jde o jednoduchou webovou aplikaci, která na výstupu vrací zdrojovou IP klienta, unikátní ID instance (abychom poznali jak to balancuje) a headery.</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: httpecho
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: httpecho
    spec:
      containers:
      - name: httpecho
        image: tkubica/httpecho
        ports:
        - containerPort: 5000</pre>
<p>Zdrojový kód Python aplikace (pokud si ji potřebujete upravit) je tady:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="python">from flask import Flask
from flask import request
import os
import signal
import uuid

app = Flask(__name__)

instanceId =  str(uuid.uuid4())

@app.route('/')
def response():
    res = "Instance ID: " + instanceId + "\n"
    res = res + "Source IP: " + request.remote_addr + "\n"
    res = res + "------\nHeaders: \n" + str(request.headers) + "\n"
    return res

if __name__ == '__main__':
    app.run(host='0.0.0.0')</pre>
<p>A Dockerfile tady:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">FROM python:3-alpine
RUN pip3 install flask
COPY app.py /opt/app.py
CMD ["python3", "/opt/app.py"]</pre>
<p>Nad deploymentem si založíme základní Service (všimněte si selektoru).</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">kind: Service
apiVersion: v1
metadata:
  name: httpecho
spec:
  selector:
    app: httpecho
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5000</pre>
<p>Služba je dostupná jen uvnitř clusteru, tak si vytvořme jednoduchý Pod na testování.</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">kind: Pod
apiVersion: v1
metadata:
  name: ubuntu
spec:
  containers:
    - name: ubuntu
      image: tutum/curl
      command: ["tail"]
      args: ["-f", "/dev/null"]</pre>
<p>A můžeme vyzkoušet. Přes kubectl exec oslovíme přes curl interní DNS jméno služby (to se rovná jejímu názvu) a uděláme to dvakrát. Měli bychom vidět, že pokaždé dostaneme odpověď z jiné instance - balancing nám tedy funguje.</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">kubectl apply -f deployment.yaml
kubectl apply -f serviceClusterIp.yaml
kubectl apply -f podUbuntu.yaml

kubectl exec ubuntu -- curl -s httpecho
Instance ID: 069106f9-26c9-404f-8eab-b69bad5da792
Source IP: 192.168.0.4
------
Headers:
User-Agent: curl/7.35.0
Host: httpecho
Accept: */*


kubectl exec ubuntu -- curl -s httpecho
Instance ID: e5780f41-bf00-4d19-b77a-a4122c266de3
Source IP: 192.168.0.4
------
Headers:
User-Agent: curl/7.35.0
Host: httpecho
Accept: */*</pre>
<p>Pokud vaše služba není stavová, je to ideální, protože zátěž bude hezky rozložena. Pokud ovšem držíte nějaký session state v paměti, budete možná potřebovat, aby Service podle zdrojové IP klienta posílala komunikaci stále na jednu instanci (na základě hash IP adresy). To se dá nastavit v definici Service takhle:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">kind: Service
apiVersion: v1
metadata:
  name: httpecho
spec:
  selector:
    app: httpecho
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5000
  sessionAffinity: ClientIP</pre>
<p>Pošleme tam nové nastavení a vyzkoušíme. Tentokrát bychom měli dostat i podruhé odpověď ze stejné instance.</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">kubectl apply -f serviceClusterIpSessionAffinity.yaml


kubectl exec ubuntu -- curl -s httpecho
Instance ID: e5780f41-bf00-4d19-b77a-a4122c266de3
Source IP: 192.168.0.4
------
Headers:
User-Agent: curl/7.35.0
Host: httpecho
Accept: */*


kubectl exec ubuntu -- curl -s httpecho
Instance ID: e5780f41-bf00-4d19-b77a-a4122c266de3
Source IP: 192.168.0.4
------
Headers:
User-Agent: curl/7.35.0
Host: httpecho
Accept: */*</pre>
<h1>Jak funguje balancování a externí přístup s NodePort</h1>
<p>Pojďme trochu proniknout do tajů balancování a pro studijní účely použijeme variantu Service v režimu NodePort. Půjde o to, že služba začne být dostupná na IP adrese každého nodu Kubernetes clusteru na nějakém vysokém portu (později si všechno zautomatizujeme použitím typu LoadBalancer, kdy se automaticky sestaví i balancing v Azure - teď se nám ale NodePort hodí pro nahlédnutí pod kapotu).</p>
<p>Protože Nody mají pouze interní IP adresu VNETu (a zatím nebudeme používat Azure LB před tím), potřebujeme vytvořit testovací VM, které bude ve stejném VNETu. Není podporováno hodit ho do Kubernetes subnetu, takže mám ve VNETu subnety dva:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">az network vnet subnet list -g aks-net --vnet-name aks-network -o table
AddressPrefix    Name        ProvisioningState    ResourceGroup
---------------  ----------  -------------------  ---------------
192.168.0.0/22   aks-subnet  Succeeded            aks-net
192.168.8.0/24   vm-subnet   Succeeded            aks-net</pre>
<p>VM založím ve vm-subnet.</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">az group create -n aks-testingvm -l westeurope
export subnetId=$(az network vnet subnet show -g aks-net --vnet-name aks-network -n vm-subnet --query id -o tsv)
az vm create -n testingvm \
    -g aks-testingvm \
    --image UbuntuLTS \
    --nsg "" \
    --admin-username tomas \
    --admin-password Azure12345678 \
    --authentication-type password \
    --size Standard_B1s \
    --subnet $subnetId</pre>
<p>Definici naší Service teď změníme na typ NodePort:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">kind: Service
apiVersion: v1
metadata:
  name: httpecho
spec:
  selector:
    app: httpecho
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5000
  type: NodePort</pre>
<p>Nejprve se podívejme kde nám Pody běží. Já mám schválně deployment do dvou instancí, ale v clusteru mám 3 nody:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">kubectl get pods -o wide
NAME                        READY     STATUS    RESTARTS   AGE       IP             NODE
httpecho-5f6b59b958-dvwpf   1/1       Running   0          2m        192.168.0.70   aks-agentpool-16216155-2
httpecho-5f6b59b958-znbcq   1/1       Running   0          2m        192.168.0.51   aks-agentpool-16216155-1</pre>
<p>Připojme se do VM a vyzkoušejme, jak se to chová. Provedeme curl na IP adresu druhého nodu (ve výpisu má číslo 1, protože první je 0) našeho Kubernetes clusteru (192.168.0.35).</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">tomas@testingvm:~$ curl 192.168.0.35:32164            
Instance ID: 0a2ef572-2a0e-4538-b8e1-7b32f5f4958d     
Source IP: 192.168.0.35                               
Node IP: 192.168.0.35                                 
------                                                
Headers:                                              
Host: 192.168.0.35:32164                              
User-Agent: curl/7.47.0                               
Accept: */*                                           
                                                      
</pre>
<p>Dostali jsme zpátky odpověď. V Host headeru vidíme, že jsme s naším curl kontaktovali 192.168.0.35. Vešli jsme do nodu a aplikovali se iptables. Výchozí implementace provádí source NAT, takže zdrojová IP paketu se mění na IP adresu nodu (hned uvidíme proč). Dostáváme odpověď z instance.</p>
<p>Zkusme to ještě jednou (nebo několikrát), jestli dostaneme i něco jiného. A je to tak:</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">tomas@testingvm:~$ curl 192.168.0.35:32164
Instance ID: d8943636-f8f2-4554-94f1-42c79ea28d49
Source IP: 192.168.0.66
Node IP: 192.168.0.66
------
Headers:
Host: 192.168.0.35:32164
User-Agent: curl/7.47.0
Accept: */*</pre>
<p>Co to znamená? Dostali jsme odpověď od jiné instance. Ta ovšem neběží na nodu 192.168.0.35, ale na jiném. Service tedy balancuje provoz na instance bez ohledu na to odkud jsme do clusteru vstoupili. Aby to fungovalo a dosáhli jsme i na instanci na jiném nodu, musí Service provést source NAT. Dostali jsme se do nodu s IP adresou 192.168.0.66, na kterém běží druhý Pod. Ten odpověděl. Můžeme vidět, že zdrojová IP je IP toho nodu zatímco Host hlavička ukazuje původní požadavek (192.168.0.4).</p>
<p>To má ještě jeden efekt. Můžeme si to klidně namířit na třetí node, na kterém žádný náš Pod neběží a přesto dostaneme odpověď!</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">tomas@testingvm:~$ curl 192.168.0.4:32164              
Instance ID: 0a2ef572-2a0e-4538-b8e1-7b32f5f4958d      
Source IP: 192.168.0.35                                
Node IP: 192.168.0.35                                  
------                                                 
Headers:                                               
Host: 192.168.0.4:32164                                
User-Agent: curl/7.47.0                                
Accept: */*</pre>
<p>Tohle je tedy výchozí chování a je velmi praktické. Ať se dostanete kamkoli, Kubernetes vás k Podu dopraví. Balancing je férový a každý Pod má stejnou šanci dostat nějakou zátěž.</p>
<p>Jsou s tím ale spojené i dva méně příjemné efekty. Tak zaprvé Service musí používat Source NAT, takže se nám ztrácí IP adresa klienta. Pokud na ni máte něco navázaného (marketingová statistika s geo-lokací, whitelisting apod.), nebude to fungovat. Druhá věc je, že i když jsme na Nodu s naším Podem, můžeme být směrováni na jiný Node - může se nám tedy zvyšovat latence. Oboje můžeme vyřešit použitím módu externalTrafficPolicy Local. To ale přináší zase jiné nevýhody. Vyzkoušejme si to.</p>
<p>Takhle bude vypadat pozměněná definice Service.</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">kind: Service
apiVersion: v1
metadata:
  name: httpecho
spec:
  selector:
    app: httpecho
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5000
  type: NodePort
  externalTrafficPolicy: Local</pre>
<p>Z testovací VM si to namíříme na druhý node (192.168.0.35).</p>
<pre class="EnlighterJSRAW" data-enlighter-language="null">tomas@testingvm:~$ curl 192.168.0.4:32164
Instance ID: 1b15917a-9ce3-4161-8e7b-985ab2479251
Source IP: 192.168.0.4
------
Headers:
Host: 192.168.0.4:32164
User-Agent: curl/7.47.0
Accept: */*</pre>
<p>I když to zkusíte několikrát, dostanete vždy dostaneme stejnou odpověď, protože na tomto nodu běží jen jedna instance Podu. Všimněte si také, že pokud teď namíříme test na node, na kterém žádná instance není, nedostaneme žádnou odpověď!</p>
<p>Výhodou řešení je, že nemáme hop navíc (lepší latence) a IP adresa klienta zůstane zachována. Na druhou stranu musíme si ohlídat na který node komunikujeme (to ale lze elegantně vyřešit s Azure Load Balancer) a rozložení zátěže není optimální. Balancovat musíte mimo cluster na úrovni Azure LB a ten bude umisťovat provoz na nody bez ohledu na to, kolik instancí na nich běží. Pokud bude na jednom 9 a na druhém nodu 1, tak oba nody dostanou 50% trafficu - takže první se bude flákat a druhý bude přetížený.</p>
<h1>Kompletní řešení s použitím infrastrukturního automatizovaného balanceru s LoadBalancer Service</h1>
<p>Už umíme vystavit službu na venkovní IP adresu nodů, ale to není kompletní řešení. Rozhodně bude chtít skutečnou jednu virtuální IP adresu pro naší službu, která bude platná ve venkovní síti nebo celém Internetu (public IP). Co pro to musíme udělat? Požádáme infrastrukturu. Můžeme vytvořit Azure Load Balancer, začlenit AKS nody do backend poolu, zjistit jaký port má služba na nodu a nastavit příslušné LB pravidlo. Nesmíme zapomenout na health probe pro případ, že by node měl nějaký problém (a zejména pro traffic policy Local). Navíc při škálování či upgradech nesmíme zapomenout vše přenastavit na nové nebo změněné nody. To není zrovna praktické, že?</p>
<p>Všechno se dá zautomatizovat a udělat elegantněji s využitím Service typu LoadBalancer. Ta využije "driver" pro prostředí, ve kterém Kubernetes běží - v našem případě Azure - a zautomatizuje kompletní nastavení balanceru.</p>
<h1>Discovery služeb běžících mimo Kubernetes cluster</h1>
<h1>Zakládání externích DNS jmen pro vaše služby</h1>
